{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2e1237",
   "metadata": {},
   "source": [
    "# O que é webscraping?\n",
    "\n",
    "Webscraping é uma técnica de extração de dados.\n",
    "\n",
    "Com ela podemos coletar dados de sites. Fazemos a ‘raspagem’ dos dados que são interessantes para nós.\n",
    "\n",
    "Existem muitas empresas que utilizam como forma de gerar recursos; agragadores de links, comparadores de preços produtos são exemplo clássicos do usos de tecnicas e programas de webscraping.\n",
    "\n",
    "Ao se obter um conjunto de dados através de um webscraping podemos armazená-los em arquivos com formatos distintos, a partir do próprio programa webscraping.\n",
    "\n",
    "Por exemplo:\n",
    "\n",
    "- salvar em um banco de dados;\n",
    "- salvar em CSV;\n",
    "- salvar em XLS;\n",
    "- salvar numa tabela dentro de um banco de dados NoSQL.\n",
    "\n",
    "# Primeiros passos para se construir uma aplicação Webscraping\n",
    "\n",
    "Para construirmos um programa webscraping são necessários alguns passos básicos:\n",
    "\n",
    "- definir a natureza dos dados/informções que queremos pesquisar;\n",
    "- pesquisar e definir os websites que receberão as pesquisas através do programa webscraping;\n",
    "- fazer o mapeamento do código html dos sites escolhidos;\n",
    "- criarmos o script e parametrizar os dados que serão recebidos.\n",
    "\n",
    "Possuir conhecimento prévio em tenologias relacionadas a websites (a menos html e css)é extremamente util para que seja possivel construir um pragama webscraping.\n",
    "\n",
    "# WebScraping é realmente necessário?\n",
    "\n",
    "Quando queremos automatizar a busca de um volume de dados/informações siginificatvios dentro de websites ou plataformas de rede social, por exemplo, um programa webscraping torna-se fortemente necessário. Vamos partir da premissa que quermos buscar o preço de um mesmo produto em diferentes plataformas de e-commerce: um webscrpaing pode fazer esse trabalho para nós a cada 2 minutos, por exemplo.\n",
    "\n",
    "Podemos, também, instruir nosso programa webscraping a buscar comentarios - dentro de redes sociais - sobre algum tema especifico. Com algumas linhas de código podemos automatizar esse processo. A única coisa necessária - depois da implementação do código - é \"rodar\" nosso script.\n",
    "\n",
    "## O que é necessário para criar um programa WebScraping\n",
    "\n",
    "Algumas ferramentas que já utilizamos para trabalahr com Python:\n",
    "\n",
    "- distribuição Ancaconda ou a instalação Python no sistema operacional do nosso computador;\n",
    "- Jupyter Notebook ou uma IDE de sua preferência;\n",
    "\n",
    "## Construindo nosso programa WebScraping\n",
    "\n",
    "Vamos implementar o primeiro webscraping utilizando o método urlopen(). Para isso, precisamos:\n",
    "\n",
    "- como primeira implementação \n",
    "- importar o módulo urllib.request para dentro de nosso programa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f52a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import da lib\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b20668ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora precisamos indicar a URL que queremos inspecionar\n",
    "url = 'https://pythonscraping.com/pages/page1.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c2efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação para a requisição e abertura do site para coletar os dados\n",
    "codeHTML = urlopen(url) # URLOpen é um método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afaa45b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando se houve a \"Captura\" do HTML\n",
    "codeHTML.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa5842",
   "metadata": {},
   "source": [
    "O retorno do método urlopen foi: o conteúdo HTML com os inumeros elementos que podemos observar em qualquer estrutura HMTL de um website simples. A partir desse momento, podemos iniciar a extração especifica do conteúdo que queremos.\n",
    "\n",
    "Para fazer o scraping desse conteudo acima usamos o módulo de dependência urllib com o método urlopen(). Esse módulo é nativo Python. É possivel, também, usarmos aquilo que é conhecido como projeto \"terceirizado\" Python. Muitos módulos de dependência utilizados em python fazem parte desse grupo. O módulo de dependencia Requests - usado amplamente em projetos webscraping - é um deles.\n",
    "\n",
    "## Utilizando urllib e requests\n",
    "\n",
    "Requests é um módulo de dependencia \"externo\" (isso significa que, ao usá-lo, criamos uma dependência para o projeto). A principal caracteristica do uso do módulo request é sua sintaxe: o código para criarmos um programa webscraping com request é implementado, geralemte, com menos linhas de código em relação a módulo urllib. Mesmo com menos linhas de código podemos chegar ao mesmo resultado. Isso com que o módulo de dependencia request seja adotado massivamente pela comunidade python.\n",
    "\n",
    "Por sua vez, urllib é, então, tecnicamente, chamada de módulo de dependencia nativo do Python: isso siginifica que a manutenção/ atualização cdo código que faz esse módulo funcionar corretamente recebe a tenção e o trabalho da mesma equipe que trabalha na linguagem no desenvolvimento e aperfeiçoamento da linguagem Python.\n",
    "\n",
    "No nosso próximo passo, utilizaremos requests e observaremos seu uso. vamos fazer, novamente, o scraping do endereço web indicado no bloco de código anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c454f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests # Importando o requests\n",
    "url = 'https://pythonscraping.com/pages/page1.html' # Indicando a URL a ser inspecionada\n",
    "code2 = requests.get(url) # Implementando a requisição\n",
    "code2.text # Apresentando o que foi capturado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0379d",
   "metadata": {},
   "source": [
    "## Manipulando o HTML\n",
    "\n",
    "#### BeautifulSoup - WebScraping Magic\n",
    "\n",
    "BeautifulSoup é um módulo de dependencia massivamente utilizado em Python. Seu uso tem como finalidade principal: **facilitar todo o processo de manipulação que planejamos aplicar ao HTML**.\n",
    "\n",
    "Com BeautifulSoup em Python é possivel extrair dados de HTML e XML de forma muito simples. Para fazer isso acessamos os elementos \"nós\" (nodes) da estrutura do HTML da pagina em que estamos aplicando nosso programa webscraping. Percorrendo as tags HTML ao invés de texto puro, facilitamos imensamente a obentação dos dados que pretendemos extrai do website pesquisado. Observe que neste passo o pragrama webscraping vai buscar o título do website. Para isso, indicamos, no nosso codigo, o elemento-tag title encontrado em qualquer estrtutura HTML de qualquer website simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80474389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as libs necessárias\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beb6cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicando a URL desejada\n",
    "url = 'http://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "add824f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo a leitura da URL indicada\n",
    "acessar = urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f12c3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando um método da Lib BS para manipular o HTML\n",
    "bs = BeautifulSoup(acessar, 'lxml') # lxml é um componente da Lib BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ca7ecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "print(bs.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2fd32",
   "metadata": {},
   "source": [
    "### Métodos Find() e FindAll()\n",
    "\n",
    "O método find() possui como caracteristica principal encontrar o primeiro elemento e retorná-lo como resultado. \n",
    "\n",
    "O método find_all() executa uma varredura em todo o documento HTML indicado pela url e retorna todas as ocorrências - que indicamos como parâmetro - encontradas. \n",
    "\n",
    "Vamos utilizar, agora, o método find() para pesquisar e capturar o primeiro elemento-tag 'h1' que ele encontrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "787c9920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"central-textlogo-wrapper\">\n",
       "<span class=\"central-textlogo__image sprite svg-Wikipedia_wordmark\">\n",
       "Wikipedia\n",
       "</span>\n",
       "<strong class=\"jsl10n localized-slogan\" data-jsl10n=\"portal.slogan\">The Free Encyclopedia</strong>\n",
       "</h1>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Procurando o primeiro H1(Cabeçalho de nível 1)\n",
    "bs.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e778a3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"central-textlogo-wrapper\">\n",
      "<span class=\"central-textlogo__image sprite svg-Wikipedia_wordmark\">\n",
      "Wikipedia\n",
      "</span>\n",
      "<strong class=\"jsl10n localized-slogan\" data-jsl10n=\"portal.slogan\">The Free Encyclopedia</strong>\n",
      "</h1>]\n"
     ]
    }
   ],
   "source": [
    "# Utilizando o FindAll\n",
    "print(bs.find_all('h1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658f447",
   "metadata": {},
   "source": [
    "Caso seja necessário encontrar um elemento - utilizando o método find_all() podemos optar por dois caminhos:\n",
    "utilizar plenamente o método o find() - como no passo anteiror;\n",
    "\n",
    "- passar o argumento limit = 1, ao método find_all()\n",
    "- Manteremos o scraping da mesma url mas mudaremos nosso elemento-tag - agora será link pesquisado. Vamos observar o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f064194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<link href=\"/static/apple-touch/wikipedia.png\" rel=\"apple-touch-icon\"/>, <link href=\"/static/favicon/wikipedia.ico\" rel=\"shortcut icon\"/>, <link href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\"/>, <link href=\"//upload.wikimedia.org\" rel=\"preconnect\"/>]\n",
      "\n",
      "\n",
      "[<link href=\"/static/apple-touch/wikipedia.png\" rel=\"apple-touch-icon\"/>]\n",
      "\n",
      "\n",
      "Número total de ocorrências na pesquisa = 4\n"
     ]
    }
   ],
   "source": [
    "# Procurando Link\n",
    "print(bs.find_all('link'))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Limitando a busca a somente encontar um elemento\n",
    "print(bs.find_all('link', limit = 1))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Exibir a contagem de ocorrências da Tag Link\n",
    "print('Número total de ocorrências na pesquisa = {}'.format(len(bs.find_all('link'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac23f0",
   "metadata": {},
   "source": [
    "## Usar elementos 'class' e 'id' para fazer extração\n",
    "\n",
    "Nesse próximo observaremos a extração buscando elementos class e id. Possivelmente, ao inspecionar qualquer website, encontraremos estes atributos. O módulo de dependencia BeautifulSoup oferece maneiras simples para acessarmos eelementos dentro do website utilizando os dois recursos.\n",
    "\n",
    "#### Procurando pela CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be58a582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img alt=\"Wikipedia\" class=\"central-featured-logo\" height=\"183\" src=\"portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png\" srcset=\"portal/wikipedia.org/assets/img/Wikipedia-logo-v2@1.5x.png 1.5x, portal/wikipedia.org/assets/img/Wikipedia-logo-v2@2x.png 2x\" width=\"200\"/>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urlAcessar = 'https://www.wikipedia.org'\n",
    "codigo = urlopen(urlAcessar)\n",
    "\n",
    "bs = BeautifulSoup(codigo, 'lxml')\n",
    "\n",
    "print(bs.find(class_='central-featured-logo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdfdc6",
   "metadata": {},
   "source": [
    "#### Procurando pelo ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc80deff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Português\n",
      "1 066 000+ artigos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Procurando o elemento ID para a lingua Portuguesa e exebindo o texto encontrado\n",
    "print(bs.find(id='js-link-box-pt').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f26870",
   "metadata": {},
   "source": [
    "## Acesso utilizando elementos CSS\n",
    "\n",
    "Seletores CSS são uma outra eficiente para acessar e extrair dados com nossa aplicação webscraping. Podemos considerar alguns exemplos de seletores CSS para mapearmos os dados que queremos acessar:\n",
    "\n",
    "- p a: aqui, estamos buscando todas as tags 'a' dentro de um parágrafo\n",
    "- div p: mapeamos todas os parágrafos (tags 'p') dentro de uma div\n",
    "- div p span: mapeamos todos os elementos 'span' dentro de um parágrafo (tag 'p') que, por sua vez, estão dentro de uma div (tag 'div')\n",
    "- table td: todos os elementos 'td' contidos dentro do elemento 'tables'\n",
    "\n",
    "**A ideia é**: selecionar/acessar seguindo a hierarquia da estrutura. Para entendermos a arquitetura dos elementos dentro do website basta mapear a árvore-HTML. \n",
    "\n",
    "Assim é possivel encontrar o elemento-alvo e pesquisar os elementos contidos dentro dele. Quanto mais nitida a informação passada ao programa webscraping (especifcação das tags) mais fácil será - para nossa aplicação - enconte o dado que buscamos. Para exemplificar o uso de seletores CSS, será utilizado método select(), acoplando como argumento o seletor CSS desejado. \n",
    "\n",
    "Observe o código abaixo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9f23e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<strong class=\"jsl10n localized-slogan\" data-jsl10n=\"portal.slogan\">The Free Encyclopedia</strong>, <strong>English</strong>, <strong>日本語</strong>, <strong>Español</strong>, <strong>Deutsch</strong>, <strong>Русский</strong>, <strong>Français</strong>, <strong>中文</strong>, <strong>Italiano</strong>, <strong>Português</strong>, <strong>Polski</strong>, <strong class=\"jsl10n\" data-jsl10n=\"portal.app-links.title\">\n",
      "<a class=\"jsl10n\" data-jsl10n=\"portal.app-links.url\" href=\"https://en.wikipedia.org/wiki/List_of_Wikipedia_mobile_applications\">\n",
      "Download Wikipedia for Android or iOS\n",
      "</a>\n",
      "</strong>]\n",
      "\n",
      "\n",
      "[<input name=\"family\" type=\"hidden\" value=\"Wikipedia\"/>, <input id=\"hiddenLanguageInput\" name=\"language\" type=\"hidden\" value=\"en\"/>, <input accesskey=\"F\" autocomplete=\"off\" autofocus=\"autofocus\" dir=\"auto\" id=\"searchInput\" name=\"search\" size=\"20\" type=\"search\"/>, <input name=\"go\" type=\"hidden\" value=\"Go\"/>]\n"
     ]
    }
   ],
   "source": [
    "# Selecionar todas as tags \"STRONG(NEGRITO)\" que estão dentro das tags div\n",
    "print(bs.select('div strong'))\n",
    "print('\\n')\n",
    "\n",
    "# Pegando todas as tags input, que estão dentro do elemento forms que por sua\n",
    "# vez estará dentro da DIV\n",
    "print(bs.select('div form input'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab06ec",
   "metadata": {},
   "source": [
    "Também é possivel utilizar seletores CSS para classes e ids, que são representados por:\n",
    "\n",
    "- (.) : ao utilizar o elemento . (ponto), o módulo de dependecia BeautifulSoup - assim como CSS - automaticamente pesquisará por classes que forma indicado com este seletor\n",
    "- (#) : para elementos 'id', o módulo de dependencia BeautifulSouo segue a mesma premissa dos elementos \"class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f7d8d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"footer-sidebar\">\n",
      "<div class=\"footer-sidebar-content\">\n",
      "<div class=\"footer-sidebar-icon sprite svg-Wikimedia-logo_black\">\n",
      "</div>\n",
      "<div class=\"footer-sidebar-text jsl10n\" data-jsl10n=\"portal.footer-description\">\n",
      "Wikipedia is hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other projects.\n",
      "</div>\n",
      "<div class=\"footer-sidebar-text\">\n",
      "<a href=\"https://donate.wikimedia.org/?utm_medium=portal&amp;utm_campaign=portalFooter&amp;utm_source=portalFooter\" target=\"_blank\">\n",
      "<span class=\"jsl10n\" data-jsl10n=\"footer-donate\">You can support our work with a donation.</span>\n",
      "</a>\n",
      "</div>\n",
      "</div>\n",
      "</div>, <div class=\"footer-sidebar app-badges\">\n",
      "<div class=\"footer-sidebar-content\">\n",
      "<div class=\"footer-sidebar-text\">\n",
      "<div class=\"footer-sidebar-icon sprite svg-wikipedia_app_tile\"></div>\n",
      "<strong class=\"jsl10n\" data-jsl10n=\"portal.app-links.title\">\n",
      "<a class=\"jsl10n\" data-jsl10n=\"portal.app-links.url\" href=\"https://en.wikipedia.org/wiki/List_of_Wikipedia_mobile_applications\">\n",
      "Download Wikipedia for Android or iOS\n",
      "</a>\n",
      "</strong>\n",
      "<p class=\"jsl10n\" data-jsl10n=\"portal.app-links.description\">\n",
      "Save your favorite articles to read offline, sync your reading lists across devices and customize your reading experience with the official Wikipedia app.\n",
      "</p>\n",
      "<ul>\n",
      "<li class=\"app-badge app-badge-android\">\n",
      "<a href=\"https://play.google.com/store/apps/details?id=org.wikipedia&amp;referrer=utm_source%3Dportal%26utm_medium%3Dbutton%26anid%3Dadmob\" rel=\"noreferrer\" target=\"_blank\">\n",
      "<span class=\"jsl10n sprite svg-badge_google_play_store\" data-jsl10n=\"portal.app-links.google-store\">Google Play Store</span>\n",
      "</a>\n",
      "</li>\n",
      "<li class=\"app-badge app-badge-ios\">\n",
      "<a href=\"https://itunes.apple.com/app/apple-store/id324715238?pt=208305&amp;ct=portal&amp;mt=8\" rel=\"noreferrer\" target=\"_blank\">\n",
      "<span class=\"jsl10n sprite svg-badge_ios_app_store\" data-jsl10n=\"portal.app-links.apple-store\">Apple App Store</span>\n",
      "</a>\n",
      "</li>\n",
      "</ul>\n",
      "</div>\n",
      "</div>\n",
      "</div>]\n",
      "\n",
      "\n",
      "[<a class=\"link-box\" data-slogan=\"A enciclopédia livre\" href=\"//pt.wikipedia.org/\" id=\"js-link-box-pt\" title=\"Português — Wikipédia — A enciclopédia livre\">\n",
      "<strong>Português</strong>\n",
      "<small><bdi dir=\"ltr\">1 066 000+</bdi> <span>artigos</span></small>\n",
      "</a>]\n"
     ]
    }
   ],
   "source": [
    "# aqui, acessamos o conteudo do elemento 'class' chamado footer-sidebar\n",
    "print(bs.select('.footer-sidebar'))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# aqui, acessamos o conteudo do elemento 'id' da opção 'Português'\n",
    "print(bs.select('#js-link-box-pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53e003",
   "metadata": {},
   "source": [
    "## Tratamento de exceções (Handling Exception)\n",
    "\n",
    "Ao criar e executar uma programa webscraping é possivel que alguns erros ocorram antes, durante ou depois da execução. Abaixo, listamos 3 possibilidades de erro que são consideradas comuns:\n",
    "\n",
    "- erro em relação ao servidor onde o site está hospedado\n",
    "- erro na estrutura do códido da aplicação webscraping\n",
    "- erro de execução quando o link de referencia - por exemplo - foi alterado\n",
    "\n",
    "Para observados - quando se trata da requisição que nossa aplicação webscraping realiza - a validação efetiva a partir do nosso código devemos estruturá-la com mais algumas instruções. é necessário importar alguns módulos que auxiliarão a execução dessa validação.\n",
    "\n",
    "Observe o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecd16b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen('https://www.wikipedia.org/')\n",
    "except HTTPError as e:\n",
    "    # Se ocorrer algium erro, exibimos ele através da chamada print()\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    # Se ocorrer algum erro com a URL exibimos ele através da chamada 'print()'\n",
    "    print('O servidor não pode ser encontrado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0bec1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL não encontrada!\n"
     ]
    }
   ],
   "source": [
    "# Gerando erros de Forma MANUAL\n",
    "try:\n",
    "    html = urlopen('https://www.wikipedia90.org/')\n",
    "except HTTPError as e:\n",
    "    # Se ocorrer algium erro, exibimos ele através da chamada print()\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    # Se ocorrer algum erro com a URL exibimos ele através da chamada 'print()'\n",
    "    print('URL não encontrada!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6249bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
